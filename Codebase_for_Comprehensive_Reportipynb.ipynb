{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarang330/_PROJECTS_AND_LABS_SARANG/blob/main/Codebase_for_Comprehensive_Reportipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Codebase and Report on aspects of Deep Learning**"
      ],
      "metadata": {
        "id": "4dkjAPWbB4pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For introduction **Deep Learning** is a subset of machine learning based on artificial neural networks with representation learning. It involves training models to recognize patterns and make decisions with minimal human intervention. Deep learning has shown remarkable performance in tasks such as **image recognition**, **speech processing**, and **natural language understanding** due to its ability to learn from vast amounts of data."
      ],
      "metadata": {
        "id": "OrHyNduQCMeV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t9IEAB-iWvE",
        "outputId": "13614dfc-3aea-40ae-e063-7b93eb4845cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "6HedIo3L9H3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LploAWlt9Jr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n"
      ],
      "metadata": {
        "id": "fgfF7u2u9NDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
      ],
      "metadata": {
        "id": "rPud7_4G9O9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 Dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBdkZHBd9Tvs",
        "outputId": "c21d6070-8422-4c99-9b4a-4e7e59991370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "nfbOYbiL9XKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "    model.train()\n",
        "    return float(num_correct) / float(num_samples) * 100\n"
      ],
      "metadata": {
        "id": "GscCzb9H9ZYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convolutional Neural Networks {CNNs}**\n"
      ],
      "metadata": {
        "id": "CrrEo3Qcmmga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs are specialized neural networks designed to process and analyze visual data, such as images and videos. They leverage spatial hierarchies in data through convolutional layers, pooling layers, and fully connected layers.\n",
        "\n",
        "1.\tConvolutional Layers\n",
        "\n",
        "2.\tPooling Layers\n",
        "\n",
        "3.\tFully Connected Layers"
      ],
      "metadata": {
        "id": "U5eiTH7tnBLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "X_D-vkwm9bJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recurrent Neural Networks {RNNs}**\n"
      ],
      "metadata": {
        "id": "KiGCsN5CnTZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs are designed to handle sequential data by maintaining a memory of previous inputs, making them suitable for tasks such as time series prediction and natural language processing.\n",
        "\n",
        "1.\tBasic RNN\n",
        "\n",
        "2.\tLong Short-Term Memory (LSTM)\n",
        "\n",
        "3.\tGated Recurrent Unit (GRU)\n"
      ],
      "metadata": {
        "id": "To0_ZTysnbuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RNN Model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size=32*32*3, hidden_size=128, num_layers=2, num_classes=10):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1, 32*32*3)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "bCErqNGR9gHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformers**"
      ],
      "metadata": {
        "id": "mNH5grU_nodb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRUs are computationally simpler and often perform similarly to LSTMs, but they may not capture dependencies as effectively in some cases.\n",
        "\n",
        "1.\tAttention Mechanism\n",
        "\n",
        "2.\tEncoder-Decoder Architecture"
      ],
      "metadata": {
        "id": "VNNgKbcjnrzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size=32*32*3, num_classes=10):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=8)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "        self.fc = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 32*32*3)\n",
        "        x = self.transformer_encoder(x.unsqueeze(0)).squeeze(0)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rt2MioyR9hns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Lw-DZHSh9jFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Training and Evaluation\n",
        "cnn = CNN()\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_model(cnn, train_loader, optimizer, criterion, num_epochs)\n",
        "cnn_acc = calculate_accuracy(test_loader, cnn)\n",
        "print(f'CNN Test Accuracy: {cnn_acc:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idvKL_q89lOK",
        "outputId": "9216a70f-484a-4493-9c19-8365c44cd337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6174\n",
            "Epoch [2/10], Loss: 0.9911\n",
            "Epoch [3/10], Loss: 0.8834\n",
            "Epoch [4/10], Loss: 0.6959\n",
            "Epoch [5/10], Loss: 0.7029\n",
            "Epoch [6/10], Loss: 0.3154\n",
            "Epoch [7/10], Loss: 0.2437\n",
            "Epoch [8/10], Loss: 0.6621\n",
            "Epoch [9/10], Loss: 0.2165\n",
            "Epoch [10/10], Loss: 0.0452\n",
            "CNN Test Accuracy: 71.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Training and Evaluation\n",
        "rnn = RNN()\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_model(rnn, train_loader, optimizer, criterion, num_epochs)\n",
        "rnn_acc = calculate_accuracy(test_loader, rnn)\n",
        "print(f'RNN Test Accuracy: {rnn_acc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLNamYvn9mAQ",
        "outputId": "479f7563-63e5-412c-d193-9ea7160993c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.0190\n",
            "Epoch [2/10], Loss: 1.1782\n",
            "Epoch [3/10], Loss: 1.4935\n",
            "Epoch [4/10], Loss: 1.8005\n",
            "Epoch [5/10], Loss: 1.4827\n",
            "Epoch [6/10], Loss: 1.4851\n",
            "Epoch [7/10], Loss: 1.7581\n",
            "Epoch [8/10], Loss: 1.1708\n",
            "Epoch [9/10], Loss: 0.8146\n",
            "Epoch [10/10], Loss: 1.4851\n",
            "RNN Test Accuracy: 48.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Training and Evaluation\n",
        "transformer = TransformerModel()\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_model(transformer, train_loader, optimizer, criterion, num_epochs)\n",
        "transformer_acc = calculate_accuracy(test_loader, transformer)\n",
        "print(f'Transformer Test Accuracy: {transformer_acc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvGh3SrB9oPJ",
        "outputId": "0d3cd74d-c901-47d9-cc60-49550328542b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.0060\n",
            "Epoch [2/10], Loss: 2.3065\n",
            "Epoch [3/10], Loss: 1.6423\n",
            "Epoch [4/10], Loss: 1.7482\n",
            "Epoch [5/10], Loss: 1.7153\n",
            "Epoch [6/10], Loss: 1.7266\n",
            "Epoch [7/10], Loss: 1.7702\n",
            "Epoch [8/10], Loss: 1.6115\n",
            "Epoch [9/10], Loss: 1.7640\n",
            "Epoch [10/10], Loss: 1.7084\n",
            "Transformer Test Accuracy: 40.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**\n",
        "\n",
        "The provided codebase demonstrates how to:\n",
        "\n",
        "Load and preprocess the CIFAR-10 dataset.\n",
        "\n",
        "Define CNN, RNN, and Transformer models.\n",
        "\n",
        "Train each model using a specified training function.\n",
        "\n",
        "Evaluate the accuracy of each model on the test dataset.\n"
      ],
      "metadata": {
        "id": "5gvvvaZDCY31"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}